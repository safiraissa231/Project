{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JKBKtChyxBA1",
        "NC3TcyCnsauq",
        "_dXnHlM0B9eO"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Package dan Importing Module"
      ],
      "metadata": {
        "id": "JKBKtChyxBA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing this package to connected our python file with our dataset that we already have in mysql\n",
        "!pip install mysql-connector-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpYsZEkQCH6Y",
        "outputId": "4e21a820-7e86-4ff1-c666-7eb514b20fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-8.0.33-cp310-cp310-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.3,>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from mysql-connector-python) (3.20.3)\n",
            "Installing collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-8.0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing mysql.connector module that's been installed\n",
        "import mysql.connector"
      ],
      "metadata": {
        "id": "_H0mLmi3wo6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing tensorflow dan tensorflow_hub(for calling pre-trained embedding tensorflow model) modules.\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "49cFFSZb5mqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing pandas module that used for processing dataframe (in here is our dataset which is in csv format)\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "aAbumxt2wm1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing math module that used for called pi (22/7) values (to scoring our cosine similiraties)\n",
        "import math"
      ],
      "metadata": {
        "id": "IVKRWRfOyg2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing regex module that used for pre-processing (cleaning text data)\n",
        "import re"
      ],
      "metadata": {
        "id": "ula9qGajxHsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing nltk module that used for pre-processing (cleaning text data)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwSbwNqz9l73",
        "outputId": "ee7bba45-9a14-4e7a-a5fd-69c9c54c06e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling Pretrained Encoder Tensorflow Model "
      ],
      "metadata": {
        "id": "NC3TcyCnsauq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a pre-trained encoder Tensorflow model that Tensorflow suggested to make\n",
        "*text similarty* model. "
      ],
      "metadata": {
        "id": "syqPKfcgx3f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aJtukHosdcP",
        "outputId": "7c609d8e-fdaa-47ab-e305-387e2f82abab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "DI-QihacB_Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the placeholders with your own values\n",
        "connection_name = 'c23df02-diskusai:asia-southeast2:diskusaidataset'\n",
        "ip_address = '34.101.70.233'\n",
        "db_name = 'diskusaidataset'\n",
        "user = 'root'\n",
        "password = 'df02'\n",
        "\n",
        "# Creating the connection string\n",
        "connection_string = f\"mysql+mysqlconnector://{user}:{password}@{ip_address}/{db_name}?unix_socket=/cloudsql/{connection_name}\"\n",
        "\n",
        "# Establishing the connection\n",
        "cnx = mysql.connector.connect(user=user, password=password, host=ip_address, database=db_name)"
      ],
      "metadata": {
        "id": "B3r1SysdCMJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing 'your_table_name' with the actual name of your table\n",
        "query = \"SELECT * FROM diskusaidataset\"\n",
        "\n",
        "# Executing the query\n",
        "cursor = cnx.cursor()\n",
        "cursor.execute(query)\n",
        "\n",
        "# Fetching all the results\n",
        "data = cursor.fetchall()\n",
        "\n",
        "# Converting the data into a Pandas DataFrame\n",
        "dataset = pd.DataFrame(data, columns=cursor.column_names)\n",
        "\n",
        "# Closeing the cursor and connection\n",
        "cursor.close()\n",
        "cnx.close()"
      ],
      "metadata": {
        "id": "V7nQvlYmCNBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning question column from special characters that don't exist in regex module\n",
        "dataset['question'] = dataset['question'].str.replace('<p>', '')\n",
        "dataset['question'] = dataset['question'].str.replace('<h2>', '')\n",
        "dataset['question'] = dataset['question'].str.replace('</h2>', '')\n",
        "dataset['question'] = dataset['question'].str.replace('<h3>', '')\n",
        "dataset['question'] = dataset['question'].str.replace('</h3>', '')\n",
        "dataset['question'] = dataset['question'].str.replace('</p>', '')\n",
        "dataset['question'] = dataset['question'].str.replace('<strong>', '')\n",
        "dataset['question'] = dataset['question'].str.replace('</strong>', '')"
      ],
      "metadata": {
        "id": "e5DTsZOf92TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning image link data from dataset \n",
        "dataset = dataset[dataset['question'].str.contains('<img src=')==False]"
      ],
      "metadata": {
        "id": "VnBkYdgu9_et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-indexing dataset after cleaning process\n",
        "dataset=dataset.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "m30WMpPF-BHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new column that contained questions links\n",
        "link_diskusi=[]\n",
        "list_course_id=(dataset['course_id'].values).tolist()\n",
        "list_id=(dataset['id'].values).tolist()\n",
        "for i in range(len(dataset)):\n",
        "  link_diskusi.append('https://www.dicoding.com/academies/'+str(list_course_id[i])+'/discussions/'+str(list_id[i]))\n",
        "dataset['link_diskusi']=link_diskusi"
      ],
      "metadata": {
        "id": "YoN_-4cHuTVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Data Text\n",
        "\n",
        "STOPWORDS1 = set(stopwords.words('indonesian'))\n",
        "STOPWORDS2 = set(stopwords.words('english'))\n",
        "MIN_WORDS = 4\n",
        "MAX_WORDS = 200\n",
        "\n",
        "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
        "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
        "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace \n",
        "\n",
        "# Defining function to clean data text using regex module\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Series of cleaning. String to lower case, remove non words characters and numbers.\n",
        "        text (str): input text\n",
        "    return (str): modified initial text\n",
        "    \"\"\"\n",
        "    text = text.lower()  # lowercase text\n",
        "    text = re.sub(PATTERN_S, ' ', text)\n",
        "    text = re.sub(PATTERN_RN, ' ', text)\n",
        "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
        "    return text\n",
        "\n",
        "# Defining tokenizer1 to clean data text using indonesian stopwords\n",
        "def tokenizer1(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS1, lemmatize=True):\n",
        "    \"\"\"\n",
        "    Lemmatize, tokenize, crop and remove stop words.\n",
        "    \"\"\"\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    tokens1 = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
        "    tokens1 = ' '.join(tokens1)\n",
        "    return tokens1    \n",
        "\n",
        "# Defining tokenizer2 to clean data text using english stopwords\n",
        "def tokenizer2(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS2, lemmatize=True):\n",
        "    \"\"\"\n",
        "    Lemmatize, tokenize, crop and remove stop words.\n",
        "    \"\"\"\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    tokens2 = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
        "    tokens2 = ' '.join(tokens2)\n",
        "    return tokens2    \n",
        "\n",
        "# Defining clean_sentences to apply all cleaning data text process to our dataset\n",
        "def clean_sentences(df):\n",
        "    \"\"\"\n",
        "    Remove irrelavant characters (in new column clean_sentence).\n",
        "    Lemmatize, tokenize words into list of words (in new column tok_lem_sentence).\n",
        "    \"\"\"\n",
        "    print('Cleaning sentences...')\n",
        "    df['question'] = df['question'].apply(clean_text)\n",
        "    df['question'] = df['question'].apply(\n",
        "        lambda x: tokenizer1(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS1, lemmatize=True))\n",
        "    df['question'] = df['question'].apply(\n",
        "        lambda x: tokenizer2(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS2, lemmatize=True))\n",
        "    return df\n",
        "\n",
        "# Applying all cleaning data text process to our dataset\n",
        "dataset = clean_sentences(dataset)\n",
        "\n",
        "# Creating tokes from our cleaned dataset\n",
        "tokens = dataset['question'].str.split()\n",
        "tokens = [word for sublist in tokens for word in sublist]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPKIKhYwi7a9",
        "outputId": "172b309d-77f2-4b6d-a16a-7923cfdd8bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning sentences...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Function That Named \"chatbot\" to Check *Cosine Similarities* Values Scores Between Our Input (diskusAI Chat-Bot's Question Input) Relatives to Questions that Already Exist in Our Dataset."
      ],
      "metadata": {
        "id": "bfXknT1fLLIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   This function has 2 parameters that need to be filled to become local variables, our dataset and diskusAI chat-bot's question input.\n",
        "*   That question input, which is in a string format, will be transformed to dataframe format.\n",
        "*   After that, this function will embed every element in question input dataframe (in here is dataframe called data) and question column from main dataset (that given by Dicoding Indonesia) using pre-trained encoder Tensorflow model.\n",
        "*   When that all elements have already embedded, this function will calculate cosine similarities values beetween question input relatives to questions that already exist in dataset, and then scoring that values.\n",
        "*   Also making list of that score, and then add that as a new column from our main dataset.\n",
        "*   After that, this function creates new list that contained top 10 scores of cosine similarities values then makes all of list' elements as index for calling top 10 modules name that the most related with question input also top 10 discussion titles and those links.\n",
        "*  This function returns list a (contained top 10 modules name related with question input), b (contained top 10 discussion titles related with question input), and c (contained top 10 discussion titles' links related with question input).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4G7_bRL8zMTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(data_set,pertanyaan):\n",
        "  xx=[]\n",
        "  xx.append(pertanyaan)\n",
        "  data = {'input': xx}  \n",
        "  data_input = pd.DataFrame(data)\n",
        "  cos_scores=[]\n",
        "  for i in range(len(data_set)):\n",
        "    sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(data_input['input'].tolist())), axis=1)\n",
        "    sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(dataset['question'][i:i+1])), axis=1)\n",
        "    cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
        "    clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
        "    scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
        "    cos_scores.append(scores)\n",
        "\n",
        "  cos_scores_list=[]\n",
        "  for i in range(len(cos_scores)):\n",
        "    cos_scores_list_tensor=cos_scores[i-1][0].numpy()\n",
        "    cos_scores_list.append(float(cos_scores_list_tensor))\n",
        "\n",
        "  dataset['Cosines Similarity Scores']=cos_scores_list\n",
        "\n",
        "  max_cos_scores_list=[]\n",
        "  a_list=[]\n",
        "  b_list=[]\n",
        "  c_list=[]\n",
        "\n",
        "  for i in range(10):\n",
        "    max_cos=max(cos_scores_list)\n",
        "    max_cos_scores_list.append(max_cos)\n",
        "    cos_scores_list.remove(max_cos)\n",
        "    dataset_cos_max = dataset.loc[dataset['Cosines Similarity Scores'] == max_cos_scores_list[i]]\n",
        "    a_list.append(dataset_cos_max['module_name'].values.tolist())\n",
        "    b_list.append(dataset_cos_max['discussion_title'].values.tolist())\n",
        "    c_list.append(dataset_cos_max['link_diskusi'].values.tolist())\n",
        "  a=[]\n",
        "  b=[]\n",
        "  c=[]\n",
        "  for i in range(10):\n",
        "    a.append(a_list[i][0])\n",
        "    b.append(b_list[i][0])\n",
        "    c.append(c_list[i][0])\n",
        "  return a, b, c"
      ],
      "metadata": {
        "id": "ghNFcfEeBxTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation"
      ],
      "metadata": {
        "id": "_dXnHlM0B9eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Selamat Datang di ChatBot DiskusAI!')\n",
        "print('-----------------------------------')\n",
        "print('-----------------------------------')\n",
        "\n",
        "while True:\n",
        "    text=str(input('Masukkan Pertanyaan Kamu (Ketik STOP Jika Ingin Mengakhiri Sesi) :'))\n",
        "    if text=='STOP':\n",
        "      print('Sampai Jumpa, Terima Kasih Telah Menggunakan DiskusAI! <3')\n",
        "      break\n",
        "    else:\n",
        "      a, b, c=chatbot(dataset,text)\n",
        "      print('-----------------------------------')\n",
        "      print('Berikut Modul yang Relevan dengan Pertanyaan Kamu: ')\n",
        "      print('-----------------------------------')\n",
        "      for j in range(len(a)):\n",
        "        print(j+1,'.',a[j])\n",
        "      print('-----------------------------------')\n",
        "      print('-----------------------------------')\n",
        "      print('Berikut Judul Diskusi yang Relevan dengan Pertanyaan Kamu: ')\n",
        "      print('-----------------------------------')\n",
        "      for k in range(len(b)):\n",
        "        print(k+1,'.',b[k])\n",
        "      print('-----------------------------------')\n",
        "      print('-----------------------------------')\n",
        "      print('Berikut Link Judul Diskusi yang Relevan dengan Pertanyaan Kamu: ')\n",
        "      print('-----------------------------------')\n",
        "      for l in range(len(c)):\n",
        "        print(l+1,'.',c[l])\n",
        "      print('-----------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xVEzFHCB7Y9",
        "outputId": "41db8e49-74bd-42bf-b260-f8c86fdc38cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selamat Datang di ChatBot DiskusAI!\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "Masukkan Pertanyaan Kamu (Ketik STOP Jika Ingin Mengakhiri Sesi) :java script python?\n",
            "-----------------------------------\n",
            "Berikut Modul yang Relevan dengan Pertanyaan Kamu: \n",
            "-----------------------------------\n",
            "1 . Kuis Coding : Function\n",
            "2 . Submission: Tugas Akhir Membuat Website\n",
            "3 . Pendahuluan Web Components\n",
            "4 . Mekanisme Belajar\n",
            "5 . If/Else Statement\n",
            "6 . Menghubungkan Berkas CSS dengan HTML\n",
            "7 . Shadow DOM\n",
            "8 . Mekanisme Belajar\n",
            "9 . Kuis Koding: Asynchronous Proses secara Berantai\n",
            "10 . Submission: Membuat Aplikasi Web dengan ES6, Custom Element, NPM, Module Bundler, dan AJAX\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "Berikut Judul Diskusi yang Relevan dengan Pertanyaan Kamu: \n",
            "-----------------------------------\n",
            "1 . fungsi minimal belom berjalan dengan baik. maaf kak ini salah dimana ya?\n",
            "2 . Apakah bila saya tidak meng ikuti webinar sertifikat yang diprakerja akan muncul?\n",
            "3 . Menerapkan webpack dll pada Multiple page html \n",
            "4 . cara menampilkan hasil ketikan\n",
            "5 . selamat sore kak, maaf kkak, ini pas aya tes logikanya udah bener , kenapa masi ada yang salah ya,\n",
            "6 . Kenapa tidak ada perubahan saat menghubungkan berkas css dengan html\n",
            "7 . bagaimana cara mendapatkan id dan value pada shadow dom?\n",
            "8 . cara buat elemen masuk div pas dibuat fixed position bagaimana ya?\n",
            "9 .  Izin bertanya pada saat kuis koding currency pas disubmit selalu salah\n",
            "10 . Merapikan tag innerHTML \n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "Berikut Link Judul Diskusi yang Relevan dengan Pertanyaan Kamu: \n",
            "-----------------------------------\n",
            "1 . https://www.dicoding.com/academies/315/discussions/103793\n",
            "2 . https://www.dicoding.com/academies/123/discussions/74076\n",
            "3 . https://www.dicoding.com/academies/163/discussions/42326\n",
            "4 . https://www.dicoding.com/academies/123/discussions/13075\n",
            "5 . https://www.dicoding.com/academies/256/discussions/201265\n",
            "6 . https://www.dicoding.com/academies/123/discussions/97694\n",
            "7 . https://www.dicoding.com/academies/163/discussions/36586\n",
            "8 . https://www.dicoding.com/academies/123/discussions/10810\n",
            "9 . https://www.dicoding.com/academies/256/discussions/103303\n",
            "10 . https://www.dicoding.com/academies/163/discussions/216615\n",
            "-----------------------------------\n",
            "Masukkan Pertanyaan Kamu (Ketik STOP Jika Ingin Mengakhiri Sesi) :STOP\n",
            "Sampai Jumpa, Terima Kasih Telah Menggunakan DiskusAI! <3\n"
          ]
        }
      ]
    }
  ]
}